{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Py-NLP\n",
    "\n",
    "In this assignment, we will focus on pre-process texts, extract text features and use them to perform text similarities and some basic sentiment analysis. The assignment consists of three parts: in the first part, you are given some functions that will constitute your text pre-processing pipeline; in the second part, you will use the defined pre-processing steps to clean some texts, you will theen vectorize the clean texts using scikit-learn and finally you will use the feature vectors to find text similarity; in the third part, you will perform some basic sentiment analysis.\n",
    "\n",
    "To complete this assignment, you need to install [spaCy](https://spacy.io/usage/models#download). You can type the following in the command prompt to install spaCy:\n",
    "\n",
    "``\n",
    "pip install -U spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "``\n",
    "\n",
    "To use spacy, you can import spaCy and the language model as follows:\n",
    "```python\n",
    "\n",
    "import spacy\n",
    "lang_model = spacy.load(\"en_core_web_sm\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U spacy\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "lang_model = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Text Pre-Processing \n",
    "\n",
    "Before we carry out any analysis with texts, we need to clean textual data and process it into more easy-to-interpret formats. This pre-processing stage can include the following steps:\n",
    "- removal of special and accented characters \n",
    "- expansion of the contractions\n",
    "- removal of punctuation \n",
    "- lowering the text case\n",
    "- removal of extra spaces\n",
    "- removal of stop words\n",
    "- lemmatization\n",
    "- tokenization\n",
    "\n",
    "You are given a list of functions that supports these operations. Take a look at them in `functions.py`. Once you're done, test the `normalize_text` function on the following sample corpus (or any other sentence of your choice):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['red bull driver quick win race', 'hey great news think mind', 'learn cool topic', 'n.g.o stand non governmental organization service']\n"
     ]
    }
   ],
   "source": [
    "sample_corpus = [\"\\n\\n\\n The Red Bull driver wasn't quick enough and he couldn't win the race.\", \n",
    "                 \"Hey that's a great news!! I think we won't mind it at all.\", \n",
    "                 \"@@You'll (learn) very cool%% topics!\", \n",
    "                 \"N.G.O. stands for non-governmental organization #service @everywhere...!!\"\n",
    "                ]\n",
    "\n",
    "\n",
    "from functions import normalize_text\n",
    "\n",
    "\n",
    "cleaned = []\n",
    "for x in sample_corpus:\n",
    "    \n",
    "    cleaned.append(normalize_text(x, lang_model, lemmatizing=True, stop_words=True))\n",
    "    \n",
    "print(cleaned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to test your function on the given corpus sample. Note that these functions process one text from some corpus and to normalize the entire process, you need to loop over the texts of the given corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Text Similarity\n",
    "\n",
    "In this part, you will use the metric cosine similarity to find the closest test to a given text (to gain an understanding of what cosine similarity means and how it is different from a regular distance metric, you can check the two slides on canvas). You will use the dataset [recipes.csv](recipes.csv). The dataset consists of the steps of 1000 recipes that were extracted from [here](https://www.kaggle.com/shuyangli94/food-com-recipes-and-user-interactions?select=RAW_recipes.csv).\n",
    "\n",
    "1. Load the [recipes.csv](data/recipes.csv) dataset. \n",
    "2. Clean and pre-process the steps of each recipe using the function you defined in the first part (`normalize_text`).\n",
    "3. Convert the steps of each recipe into a vector using [`tfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) from scikit-learn. \n",
    "4. You now have a matrix where each row represents the text of the steps of one recipe. From scikit-learn metrics, use [`cosine_similarity`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html) to compute the pairwise similarities between all recipes.\n",
    "5. Suppose a user rated the second recipe with a high rating and you want to recommend to this user 5 similar recipes. Use the obtained cosine similarity scores to find the best 5 recipes to recommend based on the highest 5 cosine similarity measures that correspond to the second recipe. (*Hint*: you can use numpy.argsort()). We used here the steps of recipes to find a recipe similar to a given one. What other features could we also include?\n",
    "\n",
    "We here slightly touched on the topic of recommendation systems. More complex recommenders could be also developed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>steps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>make a choice and proceed with recipe dependi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>preheat oven to 425 degrees f press dough int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>brown ground beef in large pot add chopped on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>place potatoes in a large pot of lightly salt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mix all ingredients&amp; boil for 2 1 / 2 hours ,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               steps\n",
       "0   make a choice and proceed with recipe dependi...\n",
       "1   preheat oven to 425 degrees f press dough int...\n",
       "2   brown ground beef in large pot add chopped on...\n",
       "3   place potatoes in a large pot of lightly salt...\n",
       "4   mix all ingredients& boil for 2 1 / 2 hours ,..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('recipes.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d315c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['steps'] = df['steps'].apply(normalize_text, args=[lang_model, ['~','@', '#', '$', '%', '^', '&', '*'], False, True, True])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "440381e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([307, 287, 661, 357, 744], dtype=int64)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "transformed_text = tfidf.fit_transform(df['steps'])\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "transform = cosine_similarity(transformed_text)\n",
    "similar_to_2 = np.argsort(transform[1:2,:]).ravel()\n",
    "similar_to_2[-6:-1]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b091785",
   "metadata": {},
   "source": [
    "Similar recipes are 590, 967, 396, 964 and 152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4878059e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'preheat oven 425 degree f press dough side 12 inch pizza pan bake 5 minute set brown cut sausage small piece whisk egg milk bowl frothy spoon sausage baked crust sprinkle cheese pour egg mixture slowly sausage cheese s p taste bake 15 20 minute egg set crust brown'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['steps'].loc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a35352bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'remove skin sausage cut 1 1 1 2 piece add oil fry sausage brown long pink drain paper towel use favorite recipe'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['steps'].loc[744]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Sentiment Analysis\n",
    "\n",
    "In this final part, you will explore some of the sentiment analysis technique. In the first question, you will use [yelp_reviews.csv](yelp_reviews.csv) dataset that consists of text reviews and their corresponding labels: 1 (for the positive review) and 0 (for the negative review). The dataset was downloaded from [UCI repository](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences). Your task is to train a linear SVM to perform text classification for sentiment analysis. In the second question, you will use [music_instr_reviews.csv](data/music_instr_reviews.csv) dataset that consists of text reviews for some musical instruments purchased from Amazon. The dataset was extracted from this [website](http://jmcauley.ucsd.edu/data/amazon/). This data comes without labels and you will perform unsupervised sentiment analysis using [spaCyTextBlob](https://spacy.io/universe/project/spacy-textblob). \n",
    "\n",
    "1. Load the [yelp_reviews.csv](data/yelp_reviews.csv) dataset. The first column consists of the reviews and the second column consists of the labels. Clean the texts of the reviews. Define a [pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline) that consists of a vectorizer (`CountVectorizer` or `tfidfVectorizer`) and a linear SVM (`SVC(kernel='linear')` or `LinearSVC()`). Perform 5-fold cross-validation ([cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score)) and compute the mean of the obtained accuracies. In this question, you don't need to change the classifier. Focus instead on changing some of the pre-processing steps (with or without lemmatization or removal of stop words for example) or change the type of vectorizer (or some of its arguments). Comment on the obtained performances. (Note: we usually have to first split the data into training and testing sets, and then apply cross-validation on the training set. For this exercise, assume the data you have is the training dataset).\n",
    "\n",
    "2. **Optional**. Load the  [music_instr_reviews.csv](data/music_instr_reviews.csv) dataset. Use [spaCyTextBlob](https://spacy.io/universe/project/spacy-textblob) to compute the polarity of each review. Examine the scores of some reviews of your choice. \n",
    "\n",
    "*Note*: sentiment scores can be used as a feature added to your data if needed. Another way to summarize the text in the textual columns is to perform topic modeling. If interested, you can check this [paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9120935/pdf/fsoc-07-886498.pdf) for a review on the topic and you can also check this technique [BERTopic](https://spacy.io/universe/project/bertopic) provided by SpaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  label\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_df = pd.read_csv('yelp_reviews.csv')\n",
    "\n",
    "yelp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4bee82fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = yelp_df['review'].apply(normalize_text, args=[lang_model, ['~','@', '#', '$', '%', '^', '&', '*'], False, True, True])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f04394f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;svm&#x27;, LinearSVC())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;svm&#x27;, LinearSVC())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('svm', LinearSVC())])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('svm', LinearSVC())\n",
    "])\n",
    "\n",
    "\n",
    "pipe.fit(X, yelp_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cf3ff917",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pipe.predict(X)\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "acc = cross_validate(pipe, X, yelp_df['label'], cv=5, scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "263adc1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7804409974777994"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(acc['test_score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0001566d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "f7c986cd421197954bb4617ce5268c1a5200f2ca959279bba175e15553fadd14"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
